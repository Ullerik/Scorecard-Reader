{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scorecard reader\n",
    "You need some folders to run this:\n",
    "* scorecards_path - path to folder with the scorecards you want analyzed\n",
    "* merged_path - temporary folder which stores images before OCR\n",
    "* merged_analyzed_path - folder which stores images after OCR\n",
    "* flagged_path - contains all scorecards which we could use OCR on for whatever reason\n",
    "* analyzed_path - all scorecards which are not flagged will be put here\n",
    "\n",
    "### OCR\n",
    "We're using the microsoft azure document intelligence to do the OCR.\n",
    "It takes images up to 4mb and returns all text (computer generated and hand written)\n",
    "\n",
    "### How it works\n",
    "1. Preprocess and merge scorecards from scorecards_path into fewer images (~40 scorecards per image). For each scorecard, the relevant information is cut out and put horisontally in a wide image. This includes event, round, competitor ID, and the 6 result boxes (5 + extra). If an image can't be identified, it will be moved into \"flagged_path\". The merged image consists of a bunch of these horisontal scorecards stacked vertically, along with an identifier \"<!>\" at the start of each line. If a scorecard can't be correctly identified, it will not be included in the merge. The function that does this stores the merged images in \"merged_path\", and returns a list of which scorecards are used.\n",
    "2. Send the merged images for OCR, move the images from \"merged_path\" to \"merged_analyzed_path\"\n",
    "3. Structure the analyzed images. We can loop through the results from the OCR and each line of text will contain text from each \"box\" in the merged image. The identifier \"<!>\" will be used to mark a new result, which is then followed by event, round, competitor ID, and the 6 result boxes. These are stored in a list, which itself is stored in a bigger list. This list is returned as \"results_list\".\n",
    "4. Now, \"scorecards_path\" and \"results_list\" contain the same information, and we save each scorecard as (unique_id)\\_(event)\\_(round)\\_id(competitor_id).jpg and a json file with the same name. The json file contains the information from results_list in a format that can be sent to WCA Live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d174f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run supporting_functions.py\n",
    "\n",
    "line_identifier = \"<!>\"\n",
    "main_path = fr\"-\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d09e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the information above in a file called \"competition_info.json\" with the following format:\n",
    "main_path = fr\"-\"\n",
    "\n",
    "# read the json file and save the information in variables\n",
    "with open(fr\"{main_path}\\competition_info.json\", \"r\") as f:\n",
    "    competition_info = json.load(f)\n",
    "competition = competition_info[\"competition\"]\n",
    "wca_live_token = competition_info[\"wca_live_token\"]\n",
    "key = competition_info[\"azure_key\"]\n",
    "scorecards_path = competition_info[\"scorecards_path\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.formrecognizer import FormRecognizerClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Set up your credentials and endpoint\n",
    "endpoint = \"https://norwayeastdoctest.cognitiveservices.azure.com/\"\n",
    "\n",
    "merged_path = main_path + fr\"\\merged\\temp\" # merged images are put here before they are analyzed\n",
    "merged_analyzed_path = main_path + fr\"\\merged\\merged_analyzed\" # merged images are put here after they are analyzed\n",
    "analyzed = main_path + fr\"\\analyzed\" # analyzed images are put here after they are analyzed\n",
    "\n",
    "# make sure the folders exist\n",
    "for path in [merged_path, merged_analyzed_path, analyzed]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "if not os.path.exists(os.path.join(analyzed, competition)):\n",
    "    os.mkdir(os.path.join(analyzed, competition))\n",
    "\n",
    "flagged_path = os.path.join(analyzed, competition, \"flagged\")\n",
    "if not os.path.exists(os.path.join(analyzed, competition, \"flagged\")):\n",
    "    os.mkdir(flagged_path)\n",
    "\n",
    "analyzed_path = os.path.join(analyzed, competition, \"analyzed\")\n",
    "if not os.path.exists(analyzed_path):\n",
    "    os.mkdir(analyzed_path)\n",
    "\n",
    "if not os.path.exists(os.path.join(analyzed, competition, \"sorted\")): # we will not use this path, but it is created for sorting the analyzed scorecards with the double checking program\n",
    "    os.mkdir(os.path.join(analyzed, competition, \"sorted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(main_path, \"event_to_id.json\"), \"r\") as f:\n",
    "    event_to_id = json.load(f)\n",
    "\n",
    "with open(os.path.join(main_path, \"event_id_max_time_allowed_cs.json\"), \"r\") as f:\n",
    "    event_id_max_time_allowed_cs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d527ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_scorecards(scorecards_path, flagged_path, merged_path, group_size = 40, downscale_factor = 0.7, compress_original = True):\n",
    "    stacked_ID_images = stack_n_images(group_size)\n",
    "    all_scorecard_paths = get_all_image_paths(scorecards_path)\n",
    "    merged_scorecard_paths = []\n",
    "    top_right_coords_list = []\n",
    "    \n",
    "    if all_scorecard_paths:\n",
    "        i = -1\n",
    "        j = len(get_all_image_paths(merged_path))\n",
    "        full_image = 0\n",
    "        for scorecard_path in all_scorecard_paths:\n",
    "            next_scorecard, top_right_coords = scorecard_to_horisontal(scorecard_path, compress_original = compress_original)\n",
    "\n",
    "            # if it could not load it properly, next_scorecard will be an int. Move to flagged folder\n",
    "            # if it is loaded properly we save the path to the scorecard\n",
    "            if isinstance(next_scorecard, int):\n",
    "                shutil.move(scorecard_path, os.path.join(flagged_path, os.path.basename(scorecard_path)))\n",
    "                continue\n",
    "            else:\n",
    "                merged_scorecard_paths += [scorecard_path]\n",
    "                top_right_coords_list += [top_right_coords]\n",
    "\n",
    "            if i == -1: # if first scorecards (i = -1)\n",
    "                full_image = next_scorecard\n",
    "                i += 1\n",
    "\n",
    "            elif i == 0: # if first scorecard of new group (i = 0)\n",
    "                # save merged image\n",
    "\n",
    "                # make sure the merged image name is unique, pad with zeros\n",
    "                merged_image_name = f'merged{j:04}.jpg'\n",
    "                j += 1\n",
    "                merged_image_path = os.path.join(merged_path, merged_image_name)\n",
    "\n",
    "                # add stacked_ID_images to the left side of the full image\n",
    "                stacked_ID_images = stacked_ID_images[:full_image.shape[0],:,:]\n",
    "                full_image = np.hstack([stacked_ID_images,full_image])\n",
    "\n",
    "                # Downscale the image\n",
    "                new_width = int(full_image.shape[1] * (downscale_factor))\n",
    "                new_height = int(full_image.shape[0] * (downscale_factor))\n",
    "                full_image = cv2.resize(full_image, (new_width, new_height))\n",
    "                \n",
    "                cv2.imwrite(merged_image_path, full_image)\n",
    "\n",
    "                # reset full image\n",
    "                full_image = next_scorecard\n",
    "\n",
    "            else: # if i > 0, add to merged image\n",
    "                # add scorecard to merged image\n",
    "                dimensions = next_scorecard.shape\n",
    "                border = np.ones((100, dimensions[1], 3), dtype=np.uint8) * 255\n",
    "                \n",
    "                full_image = np.vstack([full_image,border,next_scorecard])\n",
    "\n",
    "            i = (i + 1) % group_size\n",
    "\n",
    "        # save last image\n",
    "        # make sure the merged image name is unique\n",
    "        merged_image_name = f'merged{j:04}.jpg'\n",
    "        merged_image_path = os.path.join(merged_path, merged_image_name)\n",
    "\n",
    "        # add stacked_ID_images to the left side of the full image\n",
    "        stacked_ID_images = stacked_ID_images[:full_image.shape[0],:,:]\n",
    "        full_image = np.hstack([stacked_ID_images,full_image])\n",
    "\n",
    "        # Downscale the image\n",
    "        new_width = int(full_image.shape[1] * (downscale_factor))\n",
    "        new_height = int(full_image.shape[0] * (downscale_factor))\n",
    "        full_image = cv2.resize(full_image, (new_width, new_height))\n",
    "        \n",
    "        cv2.imwrite(merged_image_path, full_image)\n",
    "\n",
    "    return merged_scorecard_paths, top_right_coords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7910de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_merged_image(merged_image_path):\n",
    "    # Create a Form Recognizer client\n",
    "    form_recognizer_client = FormRecognizerClient(endpoint, AzureKeyCredential(key))\n",
    "    # Use the client to analyze the form\n",
    "    with open(merged_image_path, \"rb\") as f:\n",
    "        poller = form_recognizer_client.begin_recognize_content(f.read())\n",
    "    result = poller.result()\n",
    "    return result\n",
    "\n",
    "def stats_from_merged_images(merged_path, merged_analyzed_path):\n",
    "    files = os.listdir(merged_path)\n",
    "    k = len(os.listdir(merged_analyzed_path))\n",
    "    results = []\n",
    "    \n",
    "    if files:\n",
    "        # make sure we loop through the files in the correct order, meaning merged0001.jpg first, then merged0002.jpg etc.\n",
    "        files.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        for i, image in enumerate(files):\n",
    "            image_path = os.path.join(merged_path, image)\n",
    "            results.append(read_merged_image(image_path))\n",
    "            shutil.move(image_path, os.path.join(merged_analyzed_path, f'merged_analyzed{i:04}.jpg'))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1708d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def read_merged_image(merged_image_path):\n",
    "    # Create a Form Recognizer client\n",
    "    form_recognizer_client = FormRecognizerClient(endpoint, AzureKeyCredential(key))\n",
    "    # Use the client to analyze the form\n",
    "    with open(merged_image_path, \"rb\") as f:\n",
    "        poller = form_recognizer_client.begin_recognize_content(f.read())\n",
    "    result = poller.result()\n",
    "    return result\n",
    "\n",
    "def analyze_and_move_image(image_path, merged_analyzed_path, index):\n",
    "    result = read_merged_image(image_path)\n",
    "    shutil.move(image_path, os.path.join(merged_analyzed_path, f'merged_analyzed{index:04}.jpg'))\n",
    "    return result\n",
    "\n",
    "def stats_from_merged_images(merged_path, merged_analyzed_path):\n",
    "    files = os.listdir(merged_path)\n",
    "    k = len(os.listdir(merged_analyzed_path))\n",
    "    results = []\n",
    "\n",
    "    if files:\n",
    "        # Ensure correct order\n",
    "        files.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            # Use executor.map to parallelize the requests\n",
    "            results = list(executor.map(\n",
    "                lambda args: analyze_and_move_image(*args),\n",
    "                [(os.path.join(merged_path, image), merged_analyzed_path, i) for i, image in enumerate(files)]\n",
    "            ))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dff3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_attempt(attempt, event_id = \"222\", time_to_cs = True): \n",
    "    # TODO: Log changes, return them (to see what actually happens to the time)\n",
    "    flag = False\n",
    "    \n",
    "    if \"/\" in attempt:\n",
    "        flag = True\n",
    "    \n",
    "    # remove and non-digit and non-alpha characters\n",
    "    attempt = re.sub(r'[^a-zA-Z0-9.,+:=/-]', '', attempt)\n",
    "    \n",
    "    # +2/+4/...       \n",
    "    if \"=\" in attempt:\n",
    "        attempt = attempt.split(\"=\")[-1]\n",
    "    if \"-\" in attempt: # interpret - as =\n",
    "        attempt = attempt.split(\"-\")[-1]\n",
    "    \n",
    "    # change / to 1\n",
    "    attempt = attempt.replace(\"/\", \"1\") # TODO: test both 1 and ,\n",
    "    \n",
    "    # if text in result, we change to DNF or DNS (doesnt matter which, we can fix this when double checking)\n",
    "    if any(char.isalpha() for char in attempt):\n",
    "        if \"EX\" in attempt:\n",
    "            flag = True # flag extras\n",
    "            attempt = \"ex\" # we deal with extra later\n",
    "        elif \"S\" in attempt:\n",
    "            attempt = \"-2\"\n",
    "        else:\n",
    "            attempt = \"-1\"\n",
    "        return attempt, flag\n",
    "    \n",
    "    # make sure the format is mm:ss.cs\n",
    "    \n",
    "    # change , and : to .\n",
    "    attempt = attempt.replace(\",\", \".\")\n",
    "    attempt = attempt.replace(\":\",\".\")\n",
    "        \n",
    "    # change mm:ss:cs to mm:ss.cs\n",
    "    if attempt.count(\".\") > 1:\n",
    "        colon_indices = [i for i, char in enumerate(attempt) if char == '.']\n",
    "        attempt = attempt[:colon_indices[0]] + ':' + attempt[colon_indices[0] + 1:]\n",
    "    \n",
    "    # if no . in time, return 0 and flag\n",
    "    if \".\" not in attempt:\n",
    "        if len(attempt)>4 or len(attempt)<3:\n",
    "            attempt = \"0\"\n",
    "            flag = True\n",
    "            return attempt, flag\n",
    "        else: # unless 4 or 3 we flag and insert a .\n",
    "            attempt = attempt[:len(attempt)-2]+\".\"+attempt[-2:]\n",
    "            flag = True\n",
    "    \n",
    "    # remove 1's from the beginning if it's due to the box\n",
    "    if \":\" in attempt:\n",
    "        if len(attempt.split(\":\")[0])>1 and attempt[0] == \"1\":\n",
    "            attempt = attempt[1:]\n",
    "    elif \".\" in attempt:\n",
    "        if len(attempt.split(\".\")[0])>2 and attempt[0] == \"1\":\n",
    "            attempt = attempt[1:]\n",
    "    if len(attempt.split(\".\"))>1 and len(attempt.split(\".\")[1])<2:\n",
    "        attempt += \"9\"\n",
    "        flag = True # if a digit is missing after the last .\n",
    "    if attempt[0] == \":\": # damage control\n",
    "        attempt = \"1\"+attempt    \n",
    "    \n",
    "    # convert time to cs\n",
    "    \n",
    "    m, s, cs = \"0\", \"0\", \"0\"\n",
    "    try:\n",
    "        if \":\" in attempt:\n",
    "            m, scs = attempt.split(\":\")\n",
    "            s, cs = scs.split(\".\")\n",
    "        else:\n",
    "            s, cs = attempt.split(\".\")\n",
    "    except:\n",
    "        return \"0\", True\n",
    "\n",
    "    if len(m)>2 or len(s)>2 or len(cs)>3:\n",
    "        attempt = \"0\"\n",
    "        flag = True\n",
    "        return attempt, flag\n",
    "    \n",
    "    # make sure cs is 2 decimals only\n",
    "    cs = cs[:2]\n",
    "    try:\n",
    "        attempt_in_cs = str(6000*int(m) + 100*int(s) + int(cs))\n",
    "    except:\n",
    "        attempt = \"0\"\n",
    "        flag = True\n",
    "        return attempt, flag\n",
    "    \n",
    "    # Use event type to flag/skip unreasonable times (sub 4 for 3x3 for example)\n",
    "    if int(attempt_in_cs) < int(event_id_max_time_allowed_cs[event_id]):\n",
    "        attempt = \"0\"\n",
    "        attempt_in_cs =  \"0\"\n",
    "    \n",
    "    if attempt == \"0\":\n",
    "        flag = True\n",
    "        \n",
    "    if time_to_cs:\n",
    "        return attempt_in_cs, flag\n",
    "    return attempt, flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0da95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_results_list(results_list):\n",
    "    # make sure the results list is valid, i.e. has the format [event, round, competitor_id, attempt1, attempt2, attempt3, attempt4, attempt5]\n",
    "    # if not, we make event 333, round 1, competitor_id 0, and the rest 0\n",
    "\n",
    "    # if last attempt is \"ex\", we change it to 0\n",
    "    if len(results_list) and results_list[-1] == \"ex\":\n",
    "        results_list[-1] = 0\n",
    "        \n",
    "    # if \"ex\" in results_list, we change the ex to the last attempt and remove the last attempt\n",
    "    if \"ex\" in results_list:\n",
    "        results_list[results_list.index(\"ex\")] = results_list[-1]\n",
    "        results_list.pop()\n",
    "\n",
    "    if len(results_list) > 8:\n",
    "        results_list = results_list[:8]\n",
    "    if len(results_list) < 8:\n",
    "        results_list += [0 for i in range(8-len(results_list))]\n",
    "\n",
    "    # if we don't have the correct type for the first element, we save it as unknown\n",
    "    if not isinstance(results_list[0], str):\n",
    "        results_list[0] = \"unknown\"\n",
    "    \n",
    "    return results_list\n",
    "\n",
    "def structure_results(results):\n",
    "    # takes a list of results from read_merged_image and returns a list of lists of attempts\n",
    "    structured_results = []\n",
    "    results_list = []\n",
    "    i = 0\n",
    "\n",
    "    for res in results:\n",
    "        for page in res:\n",
    "            for line in page.lines:\n",
    "                text = line.text.upper()\n",
    "                text = text.replace(\"×\",\"X\") # sometimes x's is written as ×\n",
    "\n",
    "                # if \"...\" in text we skip it as it's not relevant\n",
    "                if \"...\" in text:\n",
    "                    continue\n",
    "\n",
    "                # if the text is the identifier, we start a new list. Check if text with space removed is the identifier\n",
    "                if text.replace(\" \", \"\") == line_identifier:\n",
    "                    # fix and append the last results_list\n",
    "                    results_list = fix_results_list(results_list)\n",
    "                    structured_results.append(results_list)\n",
    "                    results_list = []\n",
    "                    i = 0\n",
    "                    continue\n",
    "\n",
    "                # if i == 0, we expect the event\n",
    "                if i == 0:\n",
    "                    if text in event_to_id:\n",
    "                        results_list.append(event_to_id[text])\n",
    "                        i += 1\n",
    "                    if \"ONE\" in text:\n",
    "                        results_list.append(\"333oh\")\n",
    "                        i += 1\n",
    "                    continue\n",
    "                # if i == 1, we expect the round\n",
    "                if i == 1:\n",
    "                    # remove all non-digit characters\n",
    "                    text = ''.join(filter(str.isdigit, text))\n",
    "                    if text:\n",
    "                        if len(text) > 1:\n",
    "                            text = text[0]\n",
    "                        if int(text) > 4:\n",
    "                            text = \"0\"\n",
    "                        results_list.append(int(text))\n",
    "                    else: # assume round 1 if no round is found\n",
    "                        results_list.append(1)\n",
    "                    i += 1\n",
    "                    continue\n",
    "                # if i == 2, we expect the competitor_id\n",
    "                if i == 2:\n",
    "                    # remove all non-digit characters\n",
    "                    text = ''.join(filter(str.isdigit, text))\n",
    "                    if text:\n",
    "                        results_list.append(int(text))\n",
    "                    else: # assume competitor_id 0 if no competitor_id is found\n",
    "                        results_list.append(0)\n",
    "                    i += 1\n",
    "                    continue\n",
    "                # if i > 2, we expect the attempts\n",
    "                if i > 2:\n",
    "                    attempt, flag = structure_attempt(text) # flag can be used later\n",
    "                    results_list.append(attempt)\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "    # fix and append the last results_list\n",
    "    results_list = fix_results_list(results_list)\n",
    "    structured_results.append(results_list) \n",
    "\n",
    "    return structured_results[1:] # remove the first empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e7d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have all the paths to the scorecards that were merged in merged_scorecard_paths and the structured results in structured_results\n",
    "# we now want to move the scorecards to the analyzed folder and make json files with the same name\n",
    "# when we move the scorecards we give them a unique name (id)_(event)_(round)_(competitor_id).jpg and save the json file with the same name\n",
    "# in the json file, we save the 5 results as a list of data points with the format as below\n",
    "''' \n",
    "data = {\n",
    "  \"competitionWcaId\": competition,\n",
    "  \"eventId\": event,\n",
    "  \"roundNumber\": round,\n",
    "  \"registrantId\": competitor_id,\n",
    "  \"attemptNumber\": attempt_number,\n",
    "  \"attemptResult\": attempt_result\n",
    "}\n",
    "'''\n",
    "# we want to store the files in the analyzed folder in a subfolder with the name of the competition, and then in a subfolder named \"analyzed\".\n",
    "# the doublecheck program will store the files in a subfolder named event+round (e.g. 333r1)\n",
    "def move_to_analyzed(merged_scorecard_paths, top_right_coordinates, structured_results, analyzed_path):\n",
    "    k = len(get_all_image_paths(analyzed_path))\n",
    "    for i, path in enumerate(merged_scorecard_paths):\n",
    "        json_data = [] \n",
    "        event = structured_results[i][0]\n",
    "        round = structured_results[i][1]\n",
    "        competitor_id = structured_results[i][2]\n",
    "        for j, result in enumerate(structured_results[i][3:]):\n",
    "            json_data.append({\n",
    "                \"competitionWcaId\": competition,\n",
    "                \"eventId\": event,\n",
    "                \"roundNumber\": round,\n",
    "                \"registrantId\": competitor_id,\n",
    "                \"attemptNumber\": j+1,\n",
    "                \"attemptResult\": result\n",
    "            })\n",
    "\n",
    "        # store the original scorecard name in the json file\n",
    "        json_data.append({\n",
    "            \"originalName\": os.path.basename(path)\n",
    "        })\n",
    "        json_data.append({\n",
    "            \"topRightCoordinates\": top_right_coordinates[i] # has the format {x, ys}\n",
    "        })\n",
    "\n",
    "        # get the name of the scorecard\n",
    "        base_path = event + \"_r\" + str(round)\n",
    "        # if the event is unknown, we save it as unknown\n",
    "        if event == \"unknown\":\n",
    "            base_path = \"unknown\"\n",
    "        base_name = str(k+i) + \"_\" + base_path + \"_id\" + str(competitor_id)\n",
    "\n",
    "        # move the scorecard to the analyzed folder\n",
    "        shutil.move(path, os.path.join(analyzed_path, base_name + \".jpg\"))\n",
    "        \n",
    "        # save the json file\n",
    "        with open(os.path.join(analyzed_path, base_name + \".json\"), 'w') as outfile:\n",
    "            json.dump(json_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ae414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_scorecard_paths, top_right_coordinates = merge_scorecards(scorecards_path, flagged_path, merged_path, group_size = 60, downscale_factor = 0.65, compress_original = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0429a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = stats_from_merged_images(merged_path, merged_analyzed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a814987",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_results = structure_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d868a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The numbers below should be the same\n",
    "print(\"# scorecards (after some are moved to flagged):\",len(get_all_image_paths(scorecards_path)))\n",
    "print(\"# results:\", len(structured_results)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c57fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_to_analyzed(merged_scorecard_paths, top_right_coordinates, structured_results, analyzed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433dcf52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9587d76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 1 attempt\n",
    "{\n",
    "  \"competitionWcaId\": \"MyCompetition2023\",\n",
    "  \"eventId\": \"333\",\n",
    "  \"roundNumber\": 1,\n",
    "  \"registrantId\": 5,\n",
    "  \"attemptNumber\": 1,\n",
    "  \"attemptResult\": 1025\n",
    "}\n",
    "\n",
    "# 1 scorecard\n",
    "{\n",
    "  \"competitionWcaId\": \"MyCompetition2023\",\n",
    "  \"eventId\": \"333\",\n",
    "  \"roundNumber\": 1,\n",
    "  \"results\": [{\n",
    "    \"registrantId\": 5,\n",
    "    \"attempts\": [\n",
    "      { \"result\": 1025 },\n",
    "      { \"result\": 1100 },\n",
    "      { \"result\": 1265 },\n",
    "      { \"result\": 1010 },\n",
    "      { \"result\": 905 }\n",
    "    ]\n",
    "  }]\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d15235f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a153a518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a76d32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80309b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f16044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
